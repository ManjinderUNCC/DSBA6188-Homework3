# -*- coding: utf-8 -*-
"""Homework3-DSBA6188-retrieve_rerank_simple_wikipedia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVdFG5HxRe2xZkehpO-EsIB49e_YRwtA

# Retrieve & Re-Rank Demo over Simple Wikipedia

This examples demonstrates the Retrieve & Re-Rank Setup and allows to search over [Simple Wikipedia](https://simple.wikipedia.org/wiki/Main_Page).

You can input a query or a question. The script then uses semantic search
to find relevant passages in Simple English Wikipedia (as it is smaller and fits better in RAM).

For semantic search, we use `SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')` and retrieve
32 potentially passages that answer the input query.

Next, we use a more powerful CrossEncoder (`cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')`) that
scores the query and all retrieved passages for their relevancy. The cross-encoder further boost the performance,
especially when you search over a corpus for which the bi-encoder was not trained for.
"""

!pip install -U sentence-transformers rank_bm25

!pip install sentence-transformers torch

!pip install datasets

import json
from sentence_transformers import SentenceTransformer, CrossEncoder, util
import gzip
import os
import torch
from datasets import load_dataset

if not torch.cuda.is_available():
    print("Warning: No GPU found. Please add GPU to your notebook")


#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search
bi_encoder = SentenceTransformer('nq-distilbert-base-v1')
bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens
top_k = 32                          #Number of passages we want to retrieve with the bi-encoder

#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

ds = load_dataset("Coder-Dragon/wikipedia-movies", split='train[:1000]')
passages = []

for movie in ds:
    passage = movie["Title"] + ": " + movie["Plot"]
    passages.append(passage)

print("Passages:", len(passages))

# We encode all passages into our vector space.
corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)

# We also compare the results to lexical search (keyword search). Here, we use
# the BM25 algorithm which is implemented in the rank_bm25 package.

from rank_bm25 import BM25Okapi
from sklearn.feature_extraction import _stop_words
import string
from tqdm.autonotebook import tqdm
import numpy as np


# We lower case our text and remove stop-words from indexing
def bm25_tokenizer(text):
    tokenized_doc = []
    for token in text.lower().split():
        token = token.strip(string.punctuation)

        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:
            tokenized_doc.append(token)
    return tokenized_doc

tokenized_corpus = []
for passage in tqdm(passages):
    tokenized_corpus.append(bm25_tokenizer(passage))

bm25 = BM25Okapi(tokenized_corpus)

from tabulate import tabulate

def search(query):
    print("Input question:", query)

    # BM25 search (lexical search)
    bm25_scores = bm25.get_scores(bm25_tokenizer(query))
    top_n_bm25 = np.argsort(bm25_scores)[::-1][:5]

    bm25_results = [[f"{bm25_scores[idx]:.3f}", passages[idx]] for idx in top_n_bm25]

    # Semantic search
    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=5)

    semantic_results = [[f"{hit['score']:.3f}", passages[hit['corpus_id']]] for hit in hits[0]]

    # Reranking
    combined_scores = []
    for hit in hits[0]:
        bm25_score = bm25_scores[hit['corpus_id']]
        semantic_score = hit['score']
        combined_scores.append((0.5 * semantic_score + 0.5 * bm25_score, passages[hit['corpus_id']]))

    combined_scores = sorted(combined_scores, key=lambda x: x[0], reverse=True)

    combined_results = [[f"{score:.3f}", passage] for score, passage in combined_scores[:5]]

    # Print results in table format
    print("\nTop-5 lexical search (BM25) hits:")
    print(tabulate(bm25_results, headers=["BM25 Score", "Passage"], tablefmt="grid"))

    print("\nSemantic search hits:")
    print(tabulate(semantic_results, headers=["Semantic Score", "Passage"], tablefmt="grid"))

    print("\nTop-5 Combined hits:")
    print(tabulate(combined_results, headers=["Combined Score", "Passage"], tablefmt="grid"))

search(query="Documentaries showcasing indigenous peoples' survival and daily life in Arctic regions")

search(query="Western romance")

search(query = "Silent film about a Parisian star moving to Egypt, leaving her husband for a baron, and later reconciling after finding her family in poverty in Cairo.")

search(query = "Comedy film, office disguises, boss's daughter, elopement.")

search(query = "Lost film, Cleopatra charms Caesar, plots world rule, treasures from mummy, revels with Antony, tragic end with serpent in Alexandria.")

search(query = "Denis Gage Deane-Tanner")